{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn- A mchine learnign package in Python\n",
    "\n",
    "- Scikit-learn or sklearn is the most popular package for machine learning.\n",
    "- It provides a range of machine leraning tools such as classification, regression, clustering and dimensionality reduction through a easy API in python.\n",
    "- The dependency of this library are NumPy, SciPy and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit history of SKlearn\n",
    "- A product of google summer code project in 2007 by David Cournapeau\n",
    "- later contibuted by many people (can be seen in its github page)\n",
    "- [clik here for github](https://github.com/scikit-learn/scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "` pip install -U scikit-learn`\n",
    "or \n",
    "` Conda install scikit-learn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported tasks\n",
    "- Supervised Learning algorithms\n",
    "    - Linear Regression\n",
    "    - Support Vector Machine (SVM), \n",
    "    - Decision Tree etc.\n",
    "\n",
    "- Unsupervised Learning algorithms\n",
    " - clustering, \n",
    " - factor analysis,\n",
    "- PCA (Principal Component Analysis) etc.\n",
    "\n",
    "- Cross Validation\n",
    "- Dimensionality Reduction\n",
    "- Feature Selections\n",
    "- Ensemble methods\n",
    "\n",
    "For more informations visit documentations [here](https://scikit-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning steps or Pipeline\n",
    "- data loading\n",
    "- data preparations\n",
    "- data cleaning\n",
    "- Train- test split\n",
    "- Model Training\n",
    "- Hyper- parameter tunning\n",
    "- Model Testing\n",
    "- Model deployment\n",
    "\n",
    "\n",
    " *(All these steps are shown in figure 1 below and supported by SK-Learn)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; width:500px\" src=\"../img/Machine-learning-pipeline.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets do it step by step in SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data set loading - C0de taken from Sklearn\n",
    "from sklearn.datasets import load_iris # toy dataset in sklearn\n",
    "iris = load_iris() # call actual load iris data fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first five data samples\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.count_nonzero(y == 2))\n",
    "np.count_nonzero(y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9, 3. , 1.4, 0.2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the sake of simiplicity, we are skipping data preprocessing, otherwise we need to perform data preprocessing task in real-world applications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 4)\n",
      "(15, 4)\n",
      "(135,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "## Data spliting- Train and Test set\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data # this is the X or predicator variable for our model\n",
    "y = iris.target # this is the  y or target variable \n",
    "\n",
    "from sklearn.model_selection import train_test_split  # this function do the spliting of full data set into two: train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.1, random_state = 1\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45, 4), (105, 4), (45,), (105,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X and Y into two subset: X is divided into train and test similarly y is aslo divided into train and test\n",
    "X_test.shape, X_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
